{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTI770 - SystÃ¨mes Intelligents et Apprentissage Machine\n",
    "\n",
    "### Alessandro L. Koerich\n",
    "\n",
    "## Notebook Jupyter - 9_RNA_MLP_UpperLowercaseHandwriting_52Classes\n",
    "\n",
    "##### Ver. 1: July 2018\n",
    "##### Ver. 2: March 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from KERAS\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports from TENSOR FLOW\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the installation of Tensor Flow\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### ATTENTION ######\n",
    "# This part of the code is problematic and it may or may not work\n",
    "# If you get stucked here during an execution, you must comment the last line of this cell # TB()\n",
    "# The workaround is to open a terminal and run tensorboard in background....\n",
    "#\n",
    "# Open a Windows / Linux / MacOS terminal\n",
    "#\n",
    "# Activate the conda environment by issuing the following command:\n",
    "# Windows terminal:\n",
    "# C:> activate gti770\n",
    "# (gti770)C:>  # Your prompt should change\n",
    "# \n",
    "# Linux terminal / MacOS terminal:\n",
    "# $ source activate gti770\n",
    "# (gti770)$  # Your prompt should change\n",
    "# Run Tensorboard in background by issuing the following command:\n",
    "#(gti770)$ tensorboard --logdir=\"logs\" &\n",
    "# \n",
    "# Keep the terminal open and open a new tab in your brownser and type the address: http://127.0.0.1:6006\n",
    "\n",
    "\n",
    "def TB(cleanup=False):\n",
    "    import webbrowser\n",
    "    webbrowser.open('http://127.0.0.1:6006')\n",
    "\n",
    "    !tensorboard --logdir=\"logs\"\n",
    "\n",
    "    if cleanup:\n",
    "        !rm -R logs/\n",
    "# TB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for avoiding keras + tensorflow from using all memory:\n",
    "# Similar to the solution above, but also need to manually setup the session on Keras back-end:\n",
    "import tensorflow as tf\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 2})\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "import keras.backend.tensorflow_backend as tf_bkend\n",
    "tf_bkend.set_session(sess)\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "# NIST Train 52 Classes Uppercase + Lowecase Handwritten Characters\n",
    "# 74,880 samples for training\n",
    "# 23,670 samples for validation\n",
    "# 23,941 samples for testing\n",
    "# 108-dimensional feature vectors\n",
    "# 26 classes (A-Z uppercase characters) + 26 classes (a-z lowercase characters) \n",
    "\n",
    "TrainData = np.loadtxt('CSV_Files/Char_UpperLower52.train.csv', delimiter=' ', dtype=np.str)\n",
    "ValidData = np.loadtxt('CSV_Files/Char_UpperLower52.val.csv', delimiter=' ', dtype=np.str)\n",
    "TestData  = np.loadtxt('CSV_Files/Char_UpperLower52.test.csv' , delimiter=' ', dtype=np.str)\n",
    "\n",
    "Xtrain = TrainData[0:74779,0:108].astype(np.float)\n",
    "Ytrain = TrainData[0:74779,108:160].astype(np.int)\n",
    "\n",
    "Xvalid = ValidData[0:23669,0:108].astype(np.float)\n",
    "Yvalid = ValidData[0:23669,108:160].astype(np.int)\n",
    "\n",
    "Xtest  = TestData[0:23940,0:108].astype(np.float)\n",
    "Ytest  = TestData[0:23940,108:160].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain\n",
    "# 108 columns = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain\n",
    "# 52 columns = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xvalid = scaler.fit_transform(Xvalid)\n",
    "Xtest  = scaler.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = Ytrain.shape[1]\n",
    "input_dim   = Xtrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twolayer_model():\n",
    "    print(\"Two-Layer NN\\n\")\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=num_classes, input_dim=input_dim))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threelayer_model():\n",
    "    # create model\n",
    "    print(\"Three-Layer NN with 50 hidden neurons\\n\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=input_dim))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fourlayer_model():\n",
    "    # create model\n",
    "    print(\"Four-Layer NN with 100 and 50 hidden neurons\\n\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=input_dim))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "# Choose only one model\n",
    "\n",
    "# model = twolayer_model()\n",
    "model = threelayer_model()\n",
    "# model = fourlayer_model()\n",
    "\n",
    "# Choose the number of learning cycles and the batch size\n",
    "num_cycles = 1000\n",
    "mini_batch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create variables to Tensorboard tracing\n",
    "now = time.strftime(\"%c\")\n",
    "tbcallback = TensorBoard(log_dir='./logs/'+now, histogram_freq=0, write_graph=True, write_images=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint to store the best model \n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from livelossplot    import PlotLossesKeras\n",
    "\n",
    "# Checkpoints \n",
    "filepath   = \"weights_RNA_MLP_ULH52.best.hdf5\"\n",
    "print( filepath )\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max') \n",
    "\n",
    "callbacks_list = [tbcallback, checkpoint, PlotLossesKeras()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model (TRAIN)\n",
    "model.fit(Xtrain, Ytrain, validation_data=(Xvalid, Yvalid), epochs=num_cycles,\n",
    "          batch_size=mini_batch, callbacks=callbacks_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model (On the Training, Validation or Test dataset)\n",
    "scores = model.evaluate(Xtrain, Ytrain, verbose=0)\n",
    "print(\"Error on the training dataset: %.2f%%\" % (100-scores[1]*100))\n",
    "scores = model.evaluate(Xvalid, Yvalid, verbose=0)\n",
    "print(\"Error on the validation dataset: %.2f%%\" % (100-scores[1]*100))\n",
    "scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "print(\"Error on the test dataset: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_confusion_matrix_one_hot(model_results, truth):\n",
    "    '''model_results and truth should be for one-hot format, i.e, have >= 2 columns,\n",
    "    where truth is 0/1, and max along each row of model_results is model result\n",
    "    '''\n",
    "    \n",
    "    assert model_results.shape == truth.shape\n",
    "    num_outputs = truth.shape[1]\n",
    "    confusion_matrix = np.zeros((num_outputs, num_outputs), dtype=np.int32)\n",
    "    predictions = np.argmax(model_results,axis=1)\n",
    "    assert len(predictions)==truth.shape[0]\n",
    "\n",
    "    for actual_class in range(num_outputs):\n",
    "        idx_examples_this_class = truth[:,actual_class]==1\n",
    "        prediction_for_this_class = predictions[idx_examples_this_class]\n",
    "        for predicted_class in range(num_outputs):\n",
    "            count = np.sum(prediction_for_this_class==predicted_class)\n",
    "            confusion_matrix[actual_class, predicted_class] = count\n",
    "    assert np.sum(confusion_matrix)==len(truth)\n",
    "    assert np.sum(confusion_matrix)==np.sum(truth)\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict and show the confusion matrix (For the Validation dataset)\n",
    "predict = model.predict(Xvalid)\n",
    "confusion_matrix = get_confusion_matrix_one_hot(predict, Yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"], columns = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"])\n",
    "plt.figure(figsize = (20,20))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
