{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTI770 - Syst√®mes Intelligents et Apprentissage Machine\n",
    "\n",
    "### Alessandro L. Koerich\n",
    "\n",
    "## Notebook Jupyter - 11_Bagging_AdaBoost_UppercaseHandwriting_26Classes\n",
    "\n",
    "#### July 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import tree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "# NIST Train 26 Classes Uppercase Handwritten Characters\n",
    "# 37,440 samples for training\n",
    "# 12,092 samples for validation\n",
    "# 11,941 samples for testing\n",
    "# 108-dimensional feature vectors\n",
    "# 26 classes (A-Z uppercase characters)\n",
    "\n",
    "TrainData = np.loadtxt('CSV_Files/NISTUpperHandwritten_train.csv', delimiter=' ', dtype=np.str)\n",
    "ValidData = np.loadtxt('CSV_Files/NISTUpperHandwritten_valid.csv', delimiter=' ', dtype=np.str)\n",
    "TestData  = np.loadtxt('CSV_Files/NISTUpperHandwritten_test.csv' , delimiter=' ', dtype=np.str)\n",
    "\n",
    "Xtrain =TrainData[0:37439,0:108].astype(np.float)\n",
    "Ytrain =TrainData[0:37439,108:134].astype(np.int)\n",
    "\n",
    "Xvalid = ValidData[0:12091,0:108].astype(np.float)\n",
    "Yvalid = ValidData[0:12091,108:134].astype(np.int)\n",
    "\n",
    "Xtest  = TestData[0:11940,0:108].astype(np.float)\n",
    "Ytest  = TestData[0:11940,108:134].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "Ytrain2 = argmax(Ytrain, axis=1)\n",
    "Yvalid2 = argmax(Yvalid, axis=1)\n",
    "Ytest2  = argmax(Ytest , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xvalid = scaler.fit_transform(Xvalid)\n",
    "Xtest  = scaler.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 22, 20, ..., 10,  9, 25])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = Ytrain.shape[1]\n",
    "input_dim   = Xtrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DT_model():\n",
    "    print(\"Decision Tree\\n\")\n",
    "    # create model\n",
    "    model = tree.DecisionTreeClassifier(criterion='entropy', \n",
    "                                        max_depth=10, min_samples_leaf=10, \n",
    "                                        min_samples_split=20 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "# Choose one at each time\n",
    "model = DT_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model (TRAIN)\n",
    "model.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict the class of samples\n",
    "# Notice that we are testing on the 3 data splits\n",
    "\n",
    "Ytrain_pred = model.predict(Xtrain)\n",
    "Yvalid_pred = model.predict(Xvalid)\n",
    "Ytest_pred  = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also predict the probability of each class\n",
    "\n",
    "Ytrain_pred_prob = model.predict_proba(Xtrain)\n",
    "Yvalid_pred_prob = model.predict_proba(Xvalid)\n",
    "Ytest_pred_prob  = model.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification rate for the training dataset   = 83.8617484441358%\n",
      "Correct classification rate for the validation dataset = 77.85956496567695%\n",
      "Correct classification rate for the test dataset       = 74.69011725293132%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model (On the Training, Validation or Test dataset)\n",
    "scores = accuracy_score(Ytrain2, Ytrain_pred )\n",
    "scores2 = accuracy_score(Yvalid2, Yvalid_pred )\n",
    "scores3 = accuracy_score(Ytest2, Ytest_pred )\n",
    "\n",
    "\n",
    "print(\"Correct classification rate for the training dataset   = \"+str(scores*100)+\"%\")\n",
    "print(\"Correct classification rate for the validation dataset = \"+str(scores2*100)+\"%\")\n",
    "print(\"Correct classification rate for the test dataset       = \"+str(scores3*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.71      0.68      0.69       449\n",
      "           B       0.72      0.63      0.67       439\n",
      "           C       0.87      0.88      0.87       525\n",
      "           D       0.75      0.77      0.76       466\n",
      "           E       0.74      0.85      0.79       390\n",
      "           F       0.80      0.91      0.85       428\n",
      "           G       0.85      0.75      0.80       425\n",
      "           H       0.67      0.86      0.75       421\n",
      "           I       0.94      0.77      0.85       740\n",
      "           J       0.80      0.90      0.85       429\n",
      "           K       0.65      0.76      0.70       411\n",
      "           L       0.81      0.90      0.85       500\n",
      "           M       0.74      0.58      0.65       450\n",
      "           N       0.58      0.68      0.62       471\n",
      "           O       0.86      0.75      0.80       472\n",
      "           P       0.93      0.74      0.82       465\n",
      "           Q       0.86      0.72      0.79       450\n",
      "           R       0.62      0.68      0.65       452\n",
      "           S       0.84      0.89      0.87       467\n",
      "           T       0.94      0.93      0.94       457\n",
      "           U       0.74      0.83      0.78       472\n",
      "           V       0.73      0.88      0.80       492\n",
      "           W       0.84      0.70      0.77       467\n",
      "           X       0.79      0.74      0.76       471\n",
      "           Y       0.82      0.67      0.74       429\n",
      "           Z       0.73      0.78      0.75       453\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     12091\n",
      "   macro avg       0.78      0.78      0.78     12091\n",
      "weighted avg       0.79      0.78      0.78     12091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "print( classification_report(Yvalid2, Yvalid_pred, target_names=target_names))\n",
    "# This works, but we have labels with no predicted samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HYPERPARAMETER OPTIMIZATION FOR DECISION TREE\n",
    "\n",
    "OK, but we didn't optimize the parameters of the Decision Tree, such as:\n",
    "\n",
    "1) max_depth\n",
    "\n",
    "2) max_leaf_nodes\n",
    "\n",
    "3) min_impurity_decrease\n",
    "\n",
    "4) min_impurity_split\n",
    "\n",
    "5) min_samples_leaf\n",
    "\n",
    "6) min_samples_split\n",
    "\n",
    "7) Etc...\n",
    "\n",
    "But now, we already have a pre-defined VALIDATION dataset! So, we don't need to split the dataset and use cross-validation.\n",
    "\n",
    "We will use the hypopt Python package (pip install hypopt). It's a professional package created specifically for parameter optimization with a validation set. It works with any scikit-learn model out-of-the-box and can be used with Tensorflow, PyTorch, etc. as well.\n",
    "\n",
    "https://pypi.org/project/hypopt/1.0.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'presort', 'random_state', 'splitter'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "# Assuming you already have train, test, val sets and a model.\n",
    "from hypopt import GridSearch\n",
    "\n",
    "tuned_parameters = [{'max_depth': [15, 30, 60], \n",
    "                     'min_samples_leaf': [5, 10], \n",
    "                     'min_samples_split': [5, 10, 20]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on validation set:\n",
      "\n",
      "Test Score for Optimized Parameters: 0.8038210239020759\n"
     ]
    }
   ],
   "source": [
    "# Grid-search all parameter combinations using a validation set.\n",
    "\n",
    "tuned_model = GridSearch(model = tree.DecisionTreeClassifier(criterion='entropy'),param_grid = tuned_parameters)\n",
    "tuned_model.fit(Xtrain, Ytrain2, Xvalid, Yvalid2)\n",
    "\n",
    "print(\"Best parameters set found on validation set:\")\n",
    "print()\n",
    "print('Test Score for Optimized Parameters:', tuned_model.score(Xvalid, Yvalid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can view the best performing parameters and their scores.\n",
      "{'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "Score: 0.8038210239020759\n",
      "{'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "Score: 0.8038210239020759\n",
      "\n",
      "Verify that the lowest scoring parameters make sense.\n",
      "{'max_depth': 60, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "Score: 0.799106773633281\n",
      "{'max_depth': 30, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "Score: 0.799106773633281\n"
     ]
    }
   ],
   "source": [
    "print('We can view the best performing parameters and their scores.')\n",
    "for z in tuned_model.get_param_scores()[:2]:\n",
    "    p, s = z\n",
    "    print(p)\n",
    "    print('Score:', s)\n",
    "print()\n",
    "print('Verify that the lowest scoring parameters make sense.')\n",
    "for z in tuned_model.get_param_scores()[-2:]:\n",
    "    p, s = z\n",
    "    print(p)\n",
    "    print('Score:', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the tunes model to predict the class of samples\n",
    "\n",
    "Ytrain_pred = tuned_model.predict(Xtrain)\n",
    "Yvalid_pred = tuned_model.predict(Xvalid)\n",
    "Ytest_pred  = tuned_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also predict the probability of each class\n",
    "\n",
    "Ytrain_pred_prob = tuned_model.predict_proba(Xtrain)\n",
    "Yvalid_pred_prob = tuned_model.predict_proba(Xvalid)\n",
    "Ytest_pred_prob  = tuned_model.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification rate for the training dataset (first model) = 83.8617484441358%\n",
      "Correct classification rate for the training dataset (best model)  = 92.54787788135367%\n",
      "\n",
      "Correct classification rate for the validation dataset (first model) = 77.85956496567695%\n",
      "Correct classification rate for the validation dataset (best model)  = 80.38210239020759%\n",
      "\n",
      "Correct classification rate for the test dataset (first model) = 74.69011725293132%\n",
      "Correct classification rate for the test dataset (best model)  = 76.44891122278057%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model (On the Training, Validation or Test dataset)\n",
    "scores_tuned = accuracy_score(Ytrain2, Ytrain_pred )\n",
    "print(\"Correct classification rate for the training dataset (first model) = \"+str(scores*100)+\"%\")\n",
    "print(\"Correct classification rate for the training dataset (best model)  = \"+str(scores_tuned*100)+\"%\")\n",
    "print()\n",
    "scores_tuned = accuracy_score(Yvalid2, Yvalid_pred )\n",
    "print(\"Correct classification rate for the validation dataset (first model) = \"+str(scores2*100)+\"%\")\n",
    "print(\"Correct classification rate for the validation dataset (best model)  = \"+str(scores_tuned*100)+\"%\")\n",
    "print()\n",
    "scores_tuned = accuracy_score(Ytest2, Ytest_pred )\n",
    "print(\"Correct classification rate for the test dataset (first model) = \"+str(scores3*100)+\"%\")\n",
    "print(\"Correct classification rate for the test dataset (best model)  = \"+str(scores_tuned*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BAGGING with DECISION TREES\n",
    "\n",
    "Bagging methods are offered as a unified BaggingClassifier meta-estimator, taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets.\n",
    "\n",
    "In particular, to control the size of the subsets (in terms of samples and features):\n",
    "\n",
    "- max_samples and\n",
    "\n",
    "- max_features\n",
    "\n",
    "and\n",
    "\n",
    "- bootstrap and\n",
    "\n",
    "- bootstrap_features\n",
    "\n",
    "control whether samples and features are drawn with or without replacement.\n",
    "\n",
    "When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting:\n",
    "\n",
    "- oob_score=True.\n",
    "\n",
    "More details in: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a bagging of Decision Trees\n",
    "\n",
    "bagging = BaggingClassifier(tree.DecisionTreeClassifier(criterion='entropy'),\n",
    "                            max_samples  = 0.5,\n",
    "                            # max_features = 0.5,\n",
    "                            n_estimators = 10,\n",
    "                            n_jobs = 8,\n",
    "                            bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=0.5, n_estimators=10, n_jobs=8, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model (TRAIN)\n",
    "bagging.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict the class of samples\n",
    "\n",
    "Ytrain_pred_bagging = bagging.predict(Xtrain)\n",
    "Yvalid_pred_bagging = bagging.predict(Xvalid)\n",
    "Ytest_pred_bagging  = bagging.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification rate for the training dataset   = 99.10254013194798%\n",
      "Correct classification rate for the validation dataset = 87.98279712182615%\n",
      "\n",
      "Correct classification rate for the test dataset (first model) = 74.69011725293132%\n",
      "Correct classification rate for the test dataset (best model)  = 76.44891122278057%\n",
      "Correct classification rate for the test dataset (bagging)     = 85.80402010050251%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model (On the Training, Validation or Test dataset)\n",
    "\n",
    "scores_bagging  = accuracy_score(Ytrain2, Ytrain_pred_bagging )\n",
    "scores2_bagging = accuracy_score(Yvalid2, Yvalid_pred_bagging )\n",
    "scores3_bagging = accuracy_score(Ytest2, Ytest_pred_bagging )\n",
    "\n",
    "print(\"Correct classification rate for the training dataset   = \"+str(scores_bagging*100)+\"%\")\n",
    "print(\"Correct classification rate for the validation dataset = \"+str(scores2_bagging*100)+\"%\")\n",
    "print()\n",
    "print(\"Correct classification rate for the test dataset (first model) = \"+str(scores3*100)+\"%\")\n",
    "print(\"Correct classification rate for the test dataset (best model)  = \"+str(scores_tuned*100)+\"%\")\n",
    "print(\"Correct classification rate for the test dataset (bagging)     = \"+str(scores3_bagging*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ADABOOST with DECISION TREES\n",
    "\n",
    "The number of weak learners is controlled by the parameter n_estimators.\n",
    "\n",
    "The learning_rate parameter controls the contribution of the weak learners in the final combination.\n",
    "\n",
    "By default, weak learners are decision stumps.\n",
    "\n",
    "Different weak learners can be specified through the base_estimator parameter.\n",
    "\n",
    "The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples at a leaf min_samples_leaf in case of decision trees).\n",
    "\n",
    "More details in: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a AdaBoosting of Decision Trees\n",
    "\n",
    "AdaB= AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy'), \n",
    "                         n_estimators = 30, \n",
    "                         learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the AdaB (TRAIN)\n",
    "\n",
    "AdaB.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict the class of samples\n",
    "# Notice that we are testing the train dataset\n",
    "\n",
    "Ytrain_pred_adaboost = AdaB.predict(Xtrain)\n",
    "Yvalid_pred_adaboost = AdaB.predict(Xvalid)\n",
    "Ytest_pred_adaboost  = AdaB.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model (On the Training, Validation or Test dataset)\n",
    "\n",
    "scores_adaboost  = accuracy_score(Ytrain2, Ytrain_pred_adaboost )\n",
    "scores2_adaboost = accuracy_score(Yvalid2, Yvalid_pred_adaboost )\n",
    "scores3_adaboost = accuracy_score(Ytest2, Ytest_pred_adaboost )\n",
    "\n",
    "print(\"Correct classification rate for the training dataset   = \"+str(scores_adaboost*100)+\"%\")\n",
    "print(\"Correct classification rate for the validation dataset = \"+str(scores2_adaboost*100)+\"%\")\n",
    "print()\n",
    "print(\"Correct classification rate for the test dataset (first model) = \"+str(scores3*100)+\"%\")\n",
    "print(\"Correct classification rate for the test dataset (best model)  = \"+str(scores_tuned*100)+\"%\")\n",
    "print(\"Correct classification rate for the test dataset (bagging)     = \"+str(scores3_bagging*100)+\"%\")\n",
    "print(\"Correct classification rate for the test dataset (adaboost)    = \"+str(scores3_adaboost*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
