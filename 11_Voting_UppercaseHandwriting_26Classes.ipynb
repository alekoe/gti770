{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTI770 - Systèmes Intelligents et Apprentissage Machine\n",
    "\n",
    "Alessandro L. Koerich\n",
    "\n",
    "Notebook Jupyter - 11_Voting_UppercaseHandwriting_26Classes\n",
    "\n",
    "July 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "# NIST Train 26 Classes Uppercase Handwritten Characters\n",
    "# 37,440 samples for training\n",
    "# 12,092 samples for validation\n",
    "# 11,941 samples for testing\n",
    "# 108-dimensional feature vectors\n",
    "# 26 classes (A-Z uppercase characters)\n",
    "\n",
    "TrainData = np.loadtxt('CSV_Files/NISTUpperHandwritten_train.csv', delimiter=' ', dtype=np.str)\n",
    "ValidData = np.loadtxt('CSV_Files/NISTUpperHandwritten_valid.csv', delimiter=' ', dtype=np.str)\n",
    "TestData  = np.loadtxt('CSV_Files/NISTUpperHandwritten_test.csv' , delimiter=' ', dtype=np.str)\n",
    "\n",
    "Xtrain =TrainData[0:37439,0:108].astype(np.float)\n",
    "Ytrain =TrainData[0:37439,108:134].astype(np.int)\n",
    "\n",
    "Xvalid = ValidData[0:12091,0:108].astype(np.float)\n",
    "Yvalid = ValidData[0:12091,108:134].astype(np.int)\n",
    "\n",
    "Xtest  = TestData[0:11940,0:108].astype(np.float)\n",
    "Ytest  = TestData[0:11940,108:134].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "Ytrain2 = argmax(Ytrain, axis=1)\n",
    "Yvalid2 = argmax(Yvalid, axis=1)\n",
    "Ytest2  = argmax(Ytest , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xvalid = scaler.fit_transform(Xvalid)\n",
    "Xtest  = scaler.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = Ytrain.shape[1]\n",
    "input_dim   = Xtrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define different learning algorithm models\n",
    "from sklearn.tree import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "\n",
    "def DT_model():\n",
    "    print(\"Decision Tree\\n\")\n",
    "    # create model\n",
    "    model = tree.DecisionTreeClassifier(criterion='entropy', \n",
    "                                        max_depth=10, min_samples_leaf=10, \n",
    "                                        min_samples_split=20 )\n",
    "    return model\n",
    "\n",
    "\n",
    "def NB_model():\n",
    "    print(\"Naive Bayes Normal Distribution\\n\")\n",
    "    model = GaussianNB()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def KNN_model():\n",
    "    print(\"1-NN\\n\")\n",
    "    n_neighbors = 3\n",
    "    weights = 'uniform'\n",
    "    metric = 'euclidean'\n",
    "    algorithm = 'kd_tree'\n",
    "\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors, \n",
    "                                               weights=weights, \n",
    "                                               algorithm=algorithm, \n",
    "                                               metric=metric )\n",
    "    return model\n",
    "\n",
    "\n",
    "def linearSVM_model():\n",
    "    print(\"SVM with Linear Kernel\\n\")\n",
    "    # create model\n",
    "    model = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovo', degree=2, gamma='auto', kernel='linear',\n",
    "    max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "\n",
      "Naive Bayes Normal Distribution\n",
      "\n",
      "1-NN\n",
      "\n",
      "SVM with Linear Kernel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the four models\n",
    "\n",
    "model_DT  = DT_model()\n",
    "model_NB  = NB_model()\n",
    "model_KNN = KNN_model()\n",
    "model_SVM = linearSVM_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Building the Ensemble \n",
    "# Combination with MAJORITY VOTE (labels only)\n",
    "\n",
    "def ENSEMBLE_model_3():\n",
    "    print(\"Majority Vote of Decision Tree, Naive Bayes and SVM\\n\")\n",
    "    model = VotingClassifier(estimators = [ ('DT', model_DT), ('NB', model_NB) , ('SVM', model_SVM) ], \n",
    "                             voting = 'hard',\n",
    "                             n_jobs = 3) \n",
    "    return model\n",
    "\n",
    "\n",
    "def ENSEMBLE_model_2():\n",
    "    print(\"Majority Vote of Decision Tree and Naive Bayes\\n\")\n",
    "    model = VotingClassifier(estimators = [ ('DT', model_DT), ('NB', model_NB)], \n",
    "                             voting = 'hard',\n",
    "                             n_jobs = 2) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote of Decision Tree and Naive Bayes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the ENSEMBLE model\n",
    "\n",
    "model_ENSEMBLE  = ENSEMBLE_model_2()\n",
    "# model_ENSEMBLE  = ENSEMBLE_model_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the models (TRAIN)\n",
    "model_DT.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the models (TRAIN)\n",
    "model_NB.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='kd_tree', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the models (TRAIN)\n",
    "model_KNN.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0e3fcb302441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the models (TRAIN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_SVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gti770/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gti770/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the models (TRAIN)\n",
    "model_SVM.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('DT', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')), ('NB', GaussianNB(priors=None, var_smoothing=1e-09))],\n",
       "         flatten_transform=None, n_jobs=2, voting='hard', weights=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Ensemble (TRAIN)\n",
    "model_ENSEMBLE.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict the class of samples\n",
    "# Notice that we are testing on the 3 data splits\n",
    "\n",
    "Ytrain_pred_DT = model_DT.predict(Xtrain)\n",
    "Yvalid_pred_DT = model_DT.predict(Xvalid)\n",
    "Ytest_pred_DT  = model_DT.predict(Xtest)\n",
    "\n",
    "Ytrain_pred_NB = model_NB.predict(Xtrain)\n",
    "Yvalid_pred_NB = model_NB.predict(Xvalid)\n",
    "Ytest_pred_NB  = model_NB.predict(Xtest)\n",
    "\n",
    "# Ytrain_pred_KNN = model_KNN.predict(Xtrain)\n",
    "# Yvalid_pred_KNN = model_KNN.predict(Xvalid)\n",
    "# Ytest_pred_KNN  = model_KNN.predict(Xtest)\n",
    "\n",
    "# Ytrain_pred_SVM = model_SVM.predict(Xtrain)\n",
    "# Yvalid_pred_SVM = model_SVM.predict(Xvalid)\n",
    "# Ytest_pred_SVM  = model_SVM.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ENSEMBBLE model to predict the class of samples\n",
    "# Notice that we are testing on the 3 data splits\n",
    "\n",
    "Ytrain_pred_ENSEMBLE = model_ENSEMBLE.predict(Xtrain)\n",
    "Yvalid_pred_ENSEMBLE = model_ENSEMBLE.predict(Xvalid)\n",
    "Ytest_pred_ENSEMBLE  = model_ENSEMBLE.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for the training dataset (Decision Tree)     = 83.8617484441358%\n",
      "Classification rate for the validation dataset  (Decision Tree)  = 77.85956496567695%\n",
      "Classification rate for the test dataset (Decision Tree)         = 74.69011725293132%\n",
      "Classification rate for the training dataset (Naive Bayes)     = 83.8617484441358%\n",
      "Classification rate for the validation dataset  (Naive Bayes)  = 83.16929947895129%\n",
      "Classification rate for the test dataset (Naive Bayes)         = 81.35678391959799%\n",
      "Classification rate for the training dataset (Ensemble)     = 84.48676513795775%\n",
      "Classification rate for the validation dataset  (Ensemble)  = 80.20841948556777%\n",
      "Classification rate for the test dataset (Ensemble)         = 78.69346733668343%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Final evaluation of the model (On the Training, Validation or Test dataset)\n",
    "\n",
    "# Decision Tree\n",
    "scores_tr_DT  = accuracy_score(Ytrain2, Ytrain_pred_DT )\n",
    "scores_val_DT = accuracy_score(Yvalid2, Yvalid_pred_DT )\n",
    "scores_tst_DT = accuracy_score(Ytest2,  Ytest_pred_DT )\n",
    "\n",
    "print(\"Classification rate for the training dataset (Decision Tree)     = \"+str(scores_tr_DT*100)+\"%\")\n",
    "print(\"Classification rate for the validation dataset  (Decision Tree)  = \"+str(scores_val_DT*100)+\"%\")\n",
    "print(\"Classification rate for the test dataset (Decision Tree)         = \"+str(scores_tst_DT*100)+\"%\")\n",
    "\n",
    "# Naive Bayes Gaussian\n",
    "scores_tr_NB  = accuracy_score(Ytrain2, Ytrain_pred_DT )\n",
    "scores_val_NB = accuracy_score(Yvalid2, Yvalid_pred_NB )\n",
    "scores_tst_NB = accuracy_score(Ytest2,  Ytest_pred_NB )\n",
    "\n",
    "print(\"Classification rate for the training dataset (Naive Bayes)     = \"+str(scores_tr_NB*100)+\"%\")\n",
    "print(\"Classification rate for the validation dataset  (Naive Bayes)  = \"+str(scores_val_NB*100)+\"%\")\n",
    "print(\"Classification rate for the test dataset (Naive Bayes)         = \"+str(scores_tst_NB*100)+\"%\")\n",
    "\n",
    "# K-NN\n",
    "# scores_tr_KNN  = accuracy_score(Ytrain2, Ytrain_pred_KNN )\n",
    "# scores_val_KNN = accuracy_score(Yvalid2, Yvalid_pred_KNN )\n",
    "# scores_tst_KNN = accuracy_score(Ytest2,  Ytest_pred_KNN )\n",
    "\n",
    "# print(\"Correct classification rate for the training dataset (k-NN)     = \"+str(scores_tr_KNN*100)+\"%\")\n",
    "# print(\"Correct classification rate for the validation dataset  (k-NN)  = \"+str(scores_val_KNN*100)+\"%\")\n",
    "# print(\"Correct classification rate for the test dataset (k-NN)         = \"+str(scores_tst_KNN*100)+\"%\")\n",
    "\n",
    "# SVM\n",
    "# scores_tr_SVM  = accuracy_score(Ytrain2, Ytrain_pred_SVM )\n",
    "# scores_val_SVM = accuracy_score(Yvalid2, Yvalid_pred_SVM )\n",
    "# scores_tst_SVM = accuracy_score(Ytest2,  Ytest_pred_SVM )\n",
    "\n",
    "# print(\"Correct classification rate for the training dataset (SVM)     = \"+str(scores_tr_SVM*100)+\"%\")\n",
    "# print(\"Correct classification rate for the validation dataset  (SVM)  = \"+str(scores_val_SVM*100)+\"%\")\n",
    "# print(\"Correct classification rate for the test dataset (SVM)         = \"+str(scores_tst_SVM*100)+\"%\")\n",
    "\n",
    "# ENSEMBLE\n",
    "scores_tr_ENSEMBLE  = accuracy_score(Ytrain2, Ytrain_pred_ENSEMBLE )\n",
    "scores_val_ENSEMBLE = accuracy_score(Yvalid2, Yvalid_pred_ENSEMBLE )\n",
    "scores_tst_ENSEMBLE = accuracy_score(Ytest2,  Ytest_pred_ENSEMBLE )\n",
    "\n",
    "print(\"Classification rate for the training dataset (Ensemble)     = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n",
    "print(\"Classification rate for the validation dataset  (Ensemble)  = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n",
    "print(\"Classification rate for the test dataset (Ensemble)         = \"+str(scores_tst_ENSEMBLE*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Building the Ensemble \n",
    "# Combination with Weighted Average Probabilities (Soft Voting)¶\n",
    "\n",
    "def ENSEMBLE_model_3s():\n",
    "    \n",
    "    model = VotingClassifier(estimators = [ ('DT', model_DT), ('NB', model_NB) , ('SVM', model_SVM) ], \n",
    "                             voting = 'soft',\n",
    "                             n_jobs = 3,\n",
    "                             weights=[0.05,0.15,0.8]) \n",
    "    return model\n",
    "\n",
    "def ENSEMBLE_model_2s():\n",
    "    \n",
    "    model = VotingClassifier(estimators = [ ('DT', model_DT), ('NB', model_NB)], \n",
    "                             voting = 'soft',\n",
    "                             n_jobs = 2,\n",
    "                             weights=[0.5,0.5]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the ENSEMBLE model\n",
    "\n",
    "model_ENSEMBLE  = ENSEMBLE_model_2s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('DT', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')), ('NB', GaussianNB(priors=None))],\n",
       "         flatten_transform=None, n_jobs=2, voting='soft',\n",
       "         weights=[0.5, 0.5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Ensemble (TRAIN)\n",
    "model_ENSEMBLE.fit(Xtrain, Ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "Ytrain_pred_ENSEMBLE = model_ENSEMBLE.predict(Xtrain)\n",
    "Yvalid_pred_ENSEMBLE = model_ENSEMBLE.predict(Xvalid)\n",
    "Ytest_pred_ENSEMBLE  = model_ENSEMBLE.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for the training dataset (Ensemble)     = 88.84051390261492%\n",
      "Classification rate for the validation dataset  (Ensemble)  = 84.50913902902985%\n",
      "Classification rate for the test dataset (Ensemble)         = 82.6214405360134%\n"
     ]
    }
   ],
   "source": [
    "# ENSEMBLE\n",
    "scores_tr_ENSEMBLE  = accuracy_score(Ytrain2, Ytrain_pred_ENSEMBLE )\n",
    "scores_val_ENSEMBLE = accuracy_score(Yvalid2, Yvalid_pred_ENSEMBLE )\n",
    "scores_tst_ENSEMBLE = accuracy_score(Ytest2,  Ytest_pred_ENSEMBLE )\n",
    "\n",
    "print(\"Classification rate for the training dataset (Ensemble)     = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n",
    "print(\"Classification rate for the validation dataset  (Ensemble)  = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n",
    "print(\"Classification rate for the test dataset (Ensemble)         = \"+str(scores_tst_ENSEMBLE*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Notebook ended\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
