{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTI770 - Systèmes intelligents et apprentissage machine\n",
    "\n",
    "#### Alessandro L. Koerich\n",
    "\n",
    "## Notebook Jupyter - 6_Bayes_Normal - Optdigits Dataset\n",
    "\n",
    "##### Created: May 2018\n",
    "##### Revised: Jan 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Title of Database: Optical Recognition of Handwritten Digits\n",
    "\n",
    "\n",
    "* #### Relevant Information:\n",
    "\tWe used preprocessing programs made available by NIST to extract\n",
    "\tnormalized bitmaps of handwritten digits from a preprinted form. From\n",
    "\ta total of 43 people, 30 contributed to the training set and different\n",
    "\t13 to the test set. 32x32 bitmaps are divided into nonoverlapping \n",
    "\tblocks of 4x4 and the number of on pixels are counted in each block.\n",
    "\tThis generates an input matrix of 8x8 where each element is an \n",
    "\tinteger in the range 0..16. This reduces dimensionality and gives \n",
    "\tinvariance to small distortions.\n",
    "\n",
    "* #### Number of Instances\n",
    "\toptdigits.tra\tTraining\t3823\n",
    "\toptdigits.tes\tTesting\t\t1797\n",
    "\t\n",
    "\tThe way we used the dataset was to use half of training for \n",
    "\tactual training, one-fourth for validation and one-fourth\n",
    "\tfor writer-dependent testing. The test set was used for \n",
    "\twriter-independent testing and is the actual quality measure.\n",
    "\n",
    "* #### Number of Attributes\n",
    "\t64 inputs + 1 class attribute\n",
    "\n",
    "* #### For Each Attribute:\n",
    "\tAll input attributes are integers in the range 0..16.\n",
    "\tThe last attribute is the class code 0..9\n",
    "    \n",
    "* #### Class Distribution\n",
    "\n",
    "\n",
    "\tNo of examples in training set\n",
    "\t 0.  376\n",
    "\t 1.  389\n",
    "\t 2.  380\n",
    "\t 3.  389\n",
    "\t 4.  387\n",
    "\t 5.  376\n",
    "\t 6.  377\n",
    "\t 7.  387\n",
    "\t 8.  380\n",
    "\t 9.  382\n",
    "\n",
    "\n",
    "\tNo of examples in testing set\n",
    "\t 0.  178\n",
    "\t 1.  182\n",
    "\t 2.  177\n",
    "\t 3.  183\n",
    "\t 4.  181\n",
    "\t 5.  182\n",
    "\t 6.  181\n",
    "\t 7.  179\n",
    "\t 8.  174\n",
    "\t 9.  180\n",
    "    \n",
    " ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load TRAIN, TEST, UNKNOWN CLASS data from files\n",
    "# Numeric inputs and outputs\n",
    "num_features    = 64\n",
    "data_train      = np.loadtxt(\"CSV_Files/optdigits-train.arff.csv\", delimiter=\",\" , skiprows=1)\n",
    "data_test       = np.loadtxt(\"CSV_Files/optdigits-test.arff.csv\", delimiter=\",\" , skiprows=1)\n",
    "data_unlabelled = np.loadtxt(\"CSV_Files/optdigits-predict-nolabels.arff.csv\", delimiter=\",\" , skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape, data_test.shape, data_unlabelled.shape,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate inputs (features) from outputs (labels)\n",
    "X_train       = data_train[:,0:num_features]\n",
    "Y_train       = data_train[:,num_features] # last column = class labels\n",
    "X_test        = data_test[:,0:num_features]\n",
    "Y_test        = data_test[:,num_features] # last column = class labels\n",
    "X_unlabelled  = data_unlabelled[:,0:num_features]\n",
    "# Y_unlabelled  = ??? We don't have the labels, so there is no Y_unlabelled!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train\n",
    "# 64 columns = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train\n",
    "# last column = output = class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Naïve Bayes Documentation\n",
    "\n",
    "* http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "* http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on the Probability Distribution\n",
    "\n",
    "Now that we have our data ready to train your model, we need first to choose the appropriate probability distribution to model/represent the data.\n",
    " \n",
    "Which probability distributions should we use?\n",
    "\n",
    "    1. Bernoulli distribution: discrete features, 2 possible states (binary features)\n",
    "    \n",
    "    2. Multinomial dsitribution: discrete features, 3 or more possible states (n-ary features)\n",
    "    \n",
    "    3. Normal distribution: real-value features   \n",
    "\n",
    "If you choose:\n",
    "\n",
    "    1. BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.\n",
    "        \n",
    "    2. MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, where the data are typically represented as counts.\n",
    "        \n",
    "    3. GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the Decision Tree with the training set\n",
    "# model = BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "# model = MultinomialNB(fit_prior=True)\n",
    "model = GaussianNB()\n",
    "model = model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all parameters of the model Normal model\n",
    "# You can change all these parameters\n",
    "# See the documentation\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the class of samples\n",
    "# Notice that we are testing the train dataset\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also predict the probability of each class\n",
    "# train dataset\n",
    "Y_train_pred_prob = model.predict_proba(X_train)\n",
    "Y_train_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_digits_data = accuracy_score(Y_train, Y_train_pred )\n",
    "print(\"Correct classification rate for the training dataset = \"+str(acc_digits_data*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['0','1','2','3','4','5','6','7','8','9']\n",
    "print( classification_report(Y_train, Y_train_pred, target_names=target_names))\n",
    "# This works, but we have labels with no predicted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_digits_data = confusion_matrix(Y_train, Y_train_pred )\n",
    "cm_digits_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, aspect = 'auto')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_digits_data, classes = ['0','1','2','3','4','5','6','7','8','9'],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_digits_data, classes = ['0','1','2','3','4','5','6','7','8','9'],\n",
    "                      normalize=True,\n",
    "                      title='Confusion matrix, with normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, but TRAINING and TESTING on the same dataset does not give us a fair evaluation of the model...\n",
    "\n",
    "So, to make a fair evaluation, we need to slipt our dataset into TRAIN and VALID partitions, or use another way...\n",
    "\n",
    "1. HOLD-OUT: hold out part of the available data as a validation (or test) set\n",
    "\n",
    "2. k-FOLD CROSS VALIDATION (CV): In k-fold CV, the training set is split into k smaller sets and for each of the k “folds”:\n",
    "        \n",
    "        -- A model is trained using k-1 of the folds as training data;\n",
    "        \n",
    "        -- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "        \n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. \n",
    "\n",
    "3. LeaveOneOut Cross Validation (LOOCV): each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for n samples, we have n different training sets and n different tests set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1) HOLD-OUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using hold-out evaluation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data is already split into train and valid/test...\n",
    "# So, we don't need to use the \"train_test_split\"\n",
    "# X_train, X_valid, Y_train, Y_valid = train_test_split( X_data, Y_data, test_size=0.4, random_state=0, shuffle=True,\n",
    "#                                                     stratify=Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test dataset \n",
    "Y_test_pred      = model.predict(X_test)\n",
    "Y_test_pred_prob = model.predict_proba(X_test)\n",
    "acc_digits_test  = accuracy_score(Y_test, Y_test_pred )\n",
    "print(\"Correct classification rate for the test dataset = \"+str(acc_digits_test*100)+\"% on \"+str(Y_test.shape[0])+\" samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2) k-fold CROSS VALIDATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using k-fold cross validation (CV) evaluation\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to play with CV, let's concatenate the train and the test sets...\n",
    "# Usually we don't do that\n",
    "X_data = np.concatenate( (X_train, X_test), axis=0)\n",
    "Y_data = np.concatenate( (Y_train, Y_test), axis=0)\n",
    "print(X_data.shape, Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our model using 10-fold CV \n",
    "model = GaussianNB()\n",
    "np.set_printoptions(precision=5)\n",
    "scores = cross_val_score(model, X_data, Y_data, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k-fold cross validation accuracy: %0.5f (+/- %0.5f)\" % (scores.mean()*100, scores.std() * 2 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Leave-One-Out Cross Validation (LOOCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using leave-one-out cross validation (LOOCV) evaluation\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n data splits, where n is the total number of samples\n",
    "# 5,620 in our case\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we will train 5,620 models one on each data splits, and test the 5,620 models on 1 sample each time. \n",
    "index = 0\n",
    "acc = np.zeros(5620)\n",
    "for train_index, test_index in loo.split(X_data):\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "    Y_train, Y_test = Y_data[train_index], Y_data[test_index]\n",
    "    # Train the model on X_train,Y_train \n",
    "    model = GaussianNB()\n",
    "    model = model.fit(X_train, Y_train)\n",
    "    # Use the learned model to predict on X_test ,Y_test \n",
    "    Y_test_pred = model.predict(X_test)\n",
    "    acc_digits_valid = accuracy_score(Y_test, Y_test_pred)\n",
    "    print(\"Correct classification rate for the model \"+str(index+1)+\": \"+str(acc_digits_valid*100)+\"%\")\n",
    "    acc[index] =  acc_digits_valid\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (acc.mean()*100, acc.std() * 2 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Now, we want to use our learned model to predict the labels of new data\n",
    "\n",
    "### The unlabeled data from optdigits-predict-nolabels.arff.csv\n",
    "\n",
    "### WHAT MODEL MUST WE USE?\n",
    "\n",
    "    1. From hold-out?\n",
    "    2. From k-fold CV?\n",
    "    3. From from LOOCV?\n",
    "    4. None of them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which model will be our FINAL MODEL?\n",
    "# model = \n",
    "model = model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making prediction on the unlabelled dataset (X_unlabelled)\n",
    "Y_test_pred = model.predict(X_unlabelled)\n",
    "Y_test_pred_prob = model.predict_proba(X_unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook ended\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
